import os
import re
from typing import Optional

import httpx
from openai import OpenAI
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

load_dotenv()

app = FastAPI(title="Media Filter API", description="Help elderly identify misleading content")

# Allow cross-origin requests from mobile app
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize DeepSeek client (OpenAI-compatible)
client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)


class AnalyzeRequest(BaseModel):
    content: str  # Unified input field - can be URL or text
    # Deprecated fields (kept for backward compatibility)
    url: Optional[str] = None
    text: Optional[str] = None



class AnalyzeResponse(BaseModel):
    title: str
    verdict: str  # "reliable", "caution", "misleading"
    verdict_emoji: str
    summary: str
    details: str
    original_text: str
    score: Optional[int] = None  # Add trust score (0-10)
    input_type: str  # "url" or "text" - shows what was detected



async def extract_wechat_article(url: str) -> dict:
    """Extract content from WeChat Official Account article."""
    headers = {
        "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/8.0.38"
    }

    async with httpx.AsyncClient(follow_redirects=True, timeout=30.0) as http_client:
        response = await http_client.get(url, headers=headers)
        response.raise_for_status()
        html = response.text

    soup = BeautifulSoup(html, "lxml")

    # Extract title
    title = ""
    title_elem = soup.find("h1", class_="rich_media_title") or soup.find("h1")
    if title_elem:
        title = title_elem.get_text(strip=True)

    # Extract main content
    content = ""
    content_elem = soup.find("div", class_="rich_media_content") or soup.find("div", id="js_content")
    if content_elem:
        # Get text, preserving some structure
        content = content_elem.get_text(separator="\n", strip=True)

    # Extract author/account name
    author = ""
    author_elem = soup.find("a", class_="weui-wa-hotarea") or soup.find("span", class_="rich_media_meta_nickname")
    if author_elem:
        author = author_elem.get_text(strip=True)

    if not content:
        raise HTTPException(status_code=400, detail="æ— æ³•æå–æ–‡ç« å†…å®¹ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æ­£ç¡®")

    return {
        "title": title or "æœªçŸ¥æ ‡é¢˜",
        "content": content[:8000],  # Limit content length for LLM
        "author": author or "æœªçŸ¥æ¥æº",
    }
# UI
# shilaohua
# å¼•ç”¨åè¨€
# å¼•ç”¨äº‹ä¾‹
def detect_input_type(input_string: str) -> tuple[str, str]:
    """
    Detect if input is a URL or plain text.
    Returns: (type, normalized_input) where type is 'url' or 'text'
    """
    if not input_string:
        return ("text", "")
    
    # Remove leading/trailing whitespace
    cleaned = input_string.strip()
    
    # URL pattern detection
    url_patterns = [
        r'^https?://',  # Starts with http:// or https://
        r'^www\.',  # Starts with www.
        r'weixin\.qq\.com',  # WeChat domain
        r'mp\.weixin\.qq\.com',  # WeChat MP domain
    ]
    
    for pattern in url_patterns:
        if re.search(pattern, cleaned, re.IGNORECASE):
            # Normalize URL: add https:// if missing
            if not cleaned.startswith(('http://', 'https://')):
                cleaned = 'https://' + cleaned
            return ("url", cleaned)
    
    # Check if it looks like a URL without protocol (contains domain-like structure)
    if re.match(r'^[a-zA-Z0-9-]+\.[a-zA-Z]{2,}', cleaned):
        cleaned = 'https://' + cleaned
        return ("url", cleaned)
    
    return ("text", cleaned)

def analyze_with_llm(title: str, content: str, author: str) -> dict:
    """Use DeepSeek to analyze if the content is misleading."""

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯é‰´åˆ«ä¸“å®¶ï¼Œå¸®åŠ©è€å¹´äººè¯†åˆ«ç½‘ç»œè™šå‡ä¿¡æ¯ã€‚è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡ç« çš„å¯ä¿¡åº¦ã€‚

ã€æ–‡ç« ä¿¡æ¯ã€‘
æ ‡é¢˜ï¼š{title}
æ¥æºï¼š{author}

ã€æ–‡ç« å†…å®¹ã€‘
{content}

ã€åˆ†æç»´åº¦ã€‘è¯·ä»ä»¥ä¸‹8ä¸ªç»´åº¦è¯„ä¼°ï¼ˆæ¯é¡¹0-10åˆ†ï¼‰ï¼š

1. **ä¿¡æ¯æºå¯é æ€§**ï¼šæ¥æºæ˜¯å¦æƒå¨ï¼Ÿæ˜¯å¦æœ‰å®˜æ–¹è®¤è¯ï¼Ÿ
2. **å†…å®¹çœŸå®æ€§**ï¼šäº‹å®é™ˆè¿°æ˜¯å¦æœ‰å¯éªŒè¯çš„æ¥æºï¼Ÿæ˜¯å¦å¼•ç”¨æƒå¨æœºæ„ï¼Ÿ
3. **è¯­è¨€ç‰¹å¾**ï¼šæ˜¯å¦ä½¿ç”¨"éœ‡æƒŠ"ã€"å¿…çœ‹"ã€"é€Ÿè½¬"ã€"ä¸è½¬ä¸æ˜¯ä¸­å›½äºº"ç­‰ç…½åŠ¨æ€§è¯æ±‡ï¼Ÿ
4. **é€»è¾‘åˆç†æ€§**ï¼šè®ºè¯æ˜¯å¦ä¸¥è°¨ï¼Ÿæ˜¯å¦æœ‰æ˜æ˜¾é€»è¾‘æ¼æ´ï¼Ÿ
5. **å¥åº·ä¿¡æ¯å‡†ç¡®æ€§**ï¼šæ¶‰åŠå¥åº·å»ºè®®æ—¶ï¼Œæ˜¯å¦ç¬¦åˆç°ä»£åŒ»å­¦è®¤çŸ¥ï¼Ÿ
6. **å•†ä¸šç›®çš„**ï¼šæ˜¯å¦éšè—æ¨é”€æ„å›¾ï¼Ÿæ˜¯å¦è¯±å¯¼è´­ä¹°æˆ–æ·»åŠ è”ç³»æ–¹å¼ï¼Ÿ
7. **ç§‘å­¦ä¾æ®**ï¼šå¼•ç”¨çš„"ç ”ç©¶"ã€"ä¸“å®¶"æ˜¯å¦å…·ä½“å¯æŸ¥ï¼Ÿ
8. **æƒ…æ„Ÿæ“æ§**ï¼šæ˜¯å¦åˆ©ç”¨ææƒ§ã€æ„¤æ€’ã€ç„¦è™‘ç­‰è´Ÿé¢æƒ…ç»ªä¼ æ’­ï¼Ÿ

ã€å¸¸è§è™šå‡ä¿¡æ¯ç‰¹å¾è¯†åˆ«ã€‘
- âŒ ä¼ªç§‘å­¦å…»ç”Ÿï¼šå¦‚"ç¢±æ€§æ°´æ²»ç™Œ"ã€"ç»¿è±†æ²»ç™¾ç—…"
- âŒ å¤¸å¤§æå“ï¼šå¦‚"å†ä¸çœ‹å°±åˆ äº†"ã€"XXXå·²è¯å®è‡´ç™Œ"
- âŒ ç¼–é€ æƒå¨ï¼šå¦‚"å“ˆä½›ç ”ç©¶"ã€"å¤®è§†æŠ¥é“"ï¼ˆä½†æ— å…·ä½“å‡ºå¤„ï¼‰
- âŒ æƒ…æ„Ÿç»‘æ¶ï¼šå¦‚"è½¬å‘ç»™ä½ çˆ±çš„äºº"ã€"ä¸ºäº†å®¶äººå¥åº·"
- âŒ é˜´è°‹è®ºï¼šå¦‚"æŸæŸéšç’çœŸç›¸"ã€"å†…éƒ¨æ¶ˆæ¯"
- âŒ è½¯æ–‡æ¨é”€ï¼šæ–‡ä¸­åå¤æåŠæŸäº§å“æˆ–è”ç³»æ–¹å¼

ã€è¾“å‡ºæ ¼å¼ã€‘ï¼ˆä¸¥æ ¼æŒ‰ç…§æ­¤æ ¼å¼ï¼‰
åˆ¤å®šï¼š[å¯ä¿¡/éœ€è°¨æ…/ä¸å¯ä¿¡]
ä¿¡ä»»åº¦ï¼š[X/10åˆ†]
ç®€è¦è¯´æ˜ï¼š[ä¸€å¥è¯æ€»ç»“é—®é¢˜ï¼Œ20-30å­—]
è¯¦ç»†åˆ†æï¼š[åˆ†ç‚¹è¯´æ˜é—®é¢˜ï¼ŒåŒ…å«å…·ä½“ä¾‹è¯ï¼Œ150-250å­—]
å»ºè®®ï¼š[ç»™è€å¹´äººçš„å®ç”¨å»ºè®®ï¼Œ50å­—ä»¥å†…]

ã€æ³¨æ„äº‹é¡¹ã€‘
- ä½¿ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€ï¼Œé¿å…ä¸“ä¸šæœ¯è¯­
- ç›´æ¥ã€æ˜ç¡®åœ°æŒ‡å‡ºé—®é¢˜ï¼Œä¸æ¨¡æ£±ä¸¤å¯
- å¦‚æœæ˜¯è¯¯å¯¼ä¿¡æ¯ï¼Œå¿…é¡»æ¸…æ¥šè¯´æ˜å±å®³
- å¦‚æœå¯ä¿¡ï¼Œä¹Ÿè¦è¯´æ˜åˆ¤æ–­ä¾æ®"""

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=1500,
        temperature=0.3,  # Lower temperature for more consistent analysis
    )

    response_text = response.choices[0].message.content or ""

    # Enhanced parsing with scoring
    verdict = "caution"
    verdict_emoji = "âš ï¸"
    score = 5

    # Extract score
    score_match = re.search(r"ä¿¡ä»»åº¦[ï¼š:]\s*(\d+)", response_text)
    if score_match:
        score = int(score_match.group(1))
        if score >= 7:
            verdict = "reliable"
            verdict_emoji = "âœ…"
        elif score <= 4:
            verdict = "misleading"
            verdict_emoji = "âŒ"
        else:
            verdict = "caution"
            verdict_emoji = "âš ï¸"
    else:
        # Fallback to keyword detection
        if "å¯ä¿¡" in response_text[:80] and "ä¸å¯ä¿¡" not in response_text[:80]:
            verdict = "reliable"
            verdict_emoji = "âœ…"
        elif "ä¸å¯ä¿¡" in response_text[:80]:
            verdict = "misleading"
            verdict_emoji = "âŒ"

    # Extract structured components
    summary = ""
    details = ""
    advice = ""

    summary_match = re.search(r"ç®€è¦è¯´æ˜[ï¼š:]\s*(.+?)(?:\n|$)", response_text)
    if summary_match:
        summary = summary_match.group(1).strip()

    details_match = re.search(r"è¯¦ç»†åˆ†æ[ï¼š:]\s*(.+?)(?=å»ºè®®[ï¼š:]|\Z)", response_text, re.DOTALL)
    if details_match:
        details = details_match.group(1).strip()

    advice_match = re.search(r"å»ºè®®[ï¼š:]\s*(.+?)(?:\n|$)", response_text, re.DOTALL)
    if advice_match:
        advice = advice_match.group(1).strip()

    # Combine details and advice
    full_details = details
    if advice:
        full_details += f"\n\nğŸ’¡ å»ºè®®ï¼š{advice}"

    return {
        "verdict": verdict,
        "verdict_emoji": verdict_emoji,
        "summary": summary or "è¯·æŸ¥çœ‹è¯¦ç»†åˆ†æ",
        "details": full_details or response_text,
        "score": score,
    }

@app.get("/")
async def root():
    return {"message": "Media Filter API - å¸®åŠ©è€å¹´äººè¯†åˆ«ç½‘ç»œè™šå‡ä¿¡æ¯"}


@app.post("/analyze", response_model=AnalyzeResponse)
async def analyze_content(request: AnalyzeRequest):
    """Analyze content with auto-detection of URL or text input."""

    # Handle both new unified 'content' field and legacy 'url'/'text' fields
    input_content = request.content if hasattr(request, 'content') and request.content else None
    
    # Backward compatibility
    if not input_content:
        if request.url:
            input_content = request.url
        elif request.text:
            input_content = request.text
    
    if not input_content:
        raise HTTPException(status_code=400, detail="è¯·æä¾›æ–‡ç« é“¾æ¥æˆ–æ–‡å­—å†…å®¹")
    
    # Validate input length
    if len(input_content) > 50000:
        raise HTTPException(status_code=400, detail="å†…å®¹è¿‡é•¿ï¼Œè¯·é™åˆ¶åœ¨50000å­—ç¬¦ä»¥å†…")
    
    if len(input_content.strip()) < 10:
        raise HTTPException(status_code=400, detail="å†…å®¹è¿‡çŸ­ï¼Œè¯·æä¾›è‡³å°‘10ä¸ªå­—ç¬¦")
    
    # Auto-detect input type
    input_type, normalized_input = detect_input_type(input_content)
    
    title = "ç”¨æˆ·è¾“å…¥å†…å®¹"
    content = ""
    author = "æœªçŸ¥"
    
    if input_type == "url":
        # Check if it's a WeChat article URL
        if "mp.weixin.qq.com" in normalized_input or "weixin.qq.com" in normalized_input:
            try:
                article = await extract_wechat_article(normalized_input)
                title = article["title"]
                content = article["content"]
                author = article["author"]
            except Exception as e:
                # Fallback: if URL extraction fails, treat as text
                raise HTTPException(
                    status_code=400,
                    detail=f"æ— æ³•æå–æ–‡ç« å†…å®¹ï¼š{str(e)}ã€‚è¯·å°è¯•å¤åˆ¶æ–‡ç« å†…å®¹ç›´æ¥ç²˜è´´åˆ†æã€‚"
                )
        else:
            raise HTTPException(
                status_code=400,
                detail="ç›®å‰ä»…æ”¯æŒå¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥ã€‚å…¶ä»–é“¾æ¥è¯·å¤åˆ¶æ–‡ç« å†…å®¹åç›´æ¥ç²˜è´´åˆ†æã€‚"
            )
    else:
        # Direct text input
        content = normalized_input
        # Sanitize content
        content = re.sub(r'\s+', ' ', content)  # Normalize whitespace
        content = content.strip()
    
    # Validate content is not empty
    if not content or len(content) < 10:
        raise HTTPException(status_code=400, detail="æ–‡ç« å†…å®¹ä¸èƒ½ä¸ºç©ºæˆ–è¿‡çŸ­")
    
    # Analyze with LLM
    analysis = analyze_with_llm(title, content, author)
    
    return AnalyzeResponse(
        title=title,
        verdict=analysis["verdict"],
        verdict_emoji=analysis["verdict_emoji"],
        summary=analysis["summary"],
        details=analysis["details"],
        original_text=content[:500] + "..." if len(content) > 500 else content,
        score=analysis.get("score"),
        input_type=input_type,
    )


@app.get("/health")
async def health_check():
    return {"status": "healthy"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
